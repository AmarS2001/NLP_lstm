{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TDL Hands-On Unit 3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tnWwhX0V_5XX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "import pickle \n",
        "import numpy as np \n",
        "import os\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = [] \n",
        "\n",
        "for i in file:\n",
        "  lines.append(i)      \n",
        "\n",
        "print(\"The First Line: \", lines[0])\n",
        "print(\"The Last Line: \", lines[-1]) \n",
        "print(\"\\n\")\n",
        "\n",
        "# Cleaning data\n",
        "data = \"\"  \n",
        "\n",
        "for i in lines:\n",
        "  data = ' '. join(lines)      \n",
        "\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:360] \n",
        "\n",
        "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space \n",
        "\n",
        "new_data = data.translate(translator)  \n",
        "\n",
        "new_data[:500] \n",
        "\n",
        "z = [] \n",
        "for i in data.split():\n",
        "  if i not in z:\n",
        "    z.append(i)          \n",
        "\n",
        "data = ' '.join(z)\n",
        "\n",
        "data[:500] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "Xiu3NiQIBjJh",
        "outputId": "73f4adcc-8901-4959-ae87-38f48b5fe1db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The First Line:    Metamorphosis\n",
            "\n",
            "The Last Line:  \n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Metamorphosis Franz Kafka Translated by David Wyllie I One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" tho'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "4EPM3qasDfFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data]) \n",
        "\n",
        "# saving the tokenizer for predict function.\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb')) \n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]  \n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 \n",
        "print(vocab_size) \n",
        "\n",
        "sequences = [] \n",
        "for i in range(1, len(sequence_data)):\n",
        "  words = sequence_data[i-1:i+1]\n",
        "  sequences.append(words)\n",
        "\n",
        "print(\"The Length of sequences are: \", len(sequences)) \n",
        "\n",
        "sequences = np.array(sequences) \n",
        "\n",
        "sequences[:10] \n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "  X.append(i[0])\n",
        "  y.append(i[1]) \n",
        "\n",
        "X = np.array(X) \n",
        "y = np.array(y) \n",
        "\n",
        "print(\"The Data is: \", X[:5]) \n",
        "print(\"The responses are: \", y[:5]) \n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size) \n",
        "\n",
        "y[:5]  \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True)) \n",
        "model.add(LSTM(1000)) \n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\")) \n",
        "model.summary() \n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001)) \n",
        "model.fit(X, y, epochs=150, batch_size=64) \n",
        "model.save('netword1.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reniUIn3DdT7",
        "outputId": "db0e4eb5-121b-4f6c-e7d7-4701dc5e92b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2626\n",
            "The Length of sequences are:  3901\n",
            "The Data is:  [293 731 732 733 294]\n",
            "The responses are:  [731 732 733 294 734]\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             26260     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2626)              2628626   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,703,886\n",
            "Trainable params: 15,703,886\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61/61 [==============================] - 18s 227ms/step - loss: 7.8788\n",
            "Epoch 2/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 7.8668\n",
            "Epoch 3/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 7.8225\n",
            "Epoch 4/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 7.6351\n",
            "Epoch 5/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 7.4318\n",
            "Epoch 6/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 7.2660\n",
            "Epoch 7/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 7.1496\n",
            "Epoch 8/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 7.0255\n",
            "Epoch 9/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 6.8425\n",
            "Epoch 10/150\n",
            "61/61 [==============================] - 14s 227ms/step - loss: 6.5861\n",
            "Epoch 11/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 6.3212\n",
            "Epoch 12/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 6.0806\n",
            "Epoch 13/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 5.8840\n",
            "Epoch 14/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 5.6870\n",
            "Epoch 15/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 5.5099\n",
            "Epoch 16/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 5.3133\n",
            "Epoch 17/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 5.1635\n",
            "Epoch 18/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 4.9996\n",
            "Epoch 19/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 4.8491\n",
            "Epoch 20/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 4.7183\n",
            "Epoch 21/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 4.5884\n",
            "Epoch 22/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 4.4436\n",
            "Epoch 23/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 4.3305\n",
            "Epoch 24/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 4.1854\n",
            "Epoch 25/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 4.0063\n",
            "Epoch 26/150\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 3.8689\n",
            "Epoch 27/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 3.7638\n",
            "Epoch 28/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 3.6140\n",
            "Epoch 29/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 3.4975\n",
            "Epoch 30/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 3.3932\n",
            "Epoch 31/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 3.2878\n",
            "Epoch 32/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 3.1755\n",
            "Epoch 33/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 3.0680\n",
            "Epoch 34/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 2.9919\n",
            "Epoch 35/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 2.9102\n",
            "Epoch 36/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 2.8572\n",
            "Epoch 37/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 2.8005\n",
            "Epoch 38/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 2.7201\n",
            "Epoch 39/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 2.6243\n",
            "Epoch 40/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 2.5725\n",
            "Epoch 41/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 2.5335\n",
            "Epoch 42/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 2.4789\n",
            "Epoch 43/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 2.4404\n",
            "Epoch 44/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 2.3784\n",
            "Epoch 45/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 2.3750\n",
            "Epoch 46/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 2.3314\n",
            "Epoch 47/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 2.2616\n",
            "Epoch 48/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 2.2191\n",
            "Epoch 49/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 2.1573\n",
            "Epoch 50/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 2.1155\n",
            "Epoch 51/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 2.0983\n",
            "Epoch 52/150\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 2.0802\n",
            "Epoch 53/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 2.0531\n",
            "Epoch 54/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 2.0282\n",
            "Epoch 55/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 2.0318\n",
            "Epoch 56/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.9894\n",
            "Epoch 57/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.9228\n",
            "Epoch 58/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.9118\n",
            "Epoch 59/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.8966\n",
            "Epoch 60/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.8808\n",
            "Epoch 61/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.8262\n",
            "Epoch 62/150\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.8365\n",
            "Epoch 63/150\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.8412\n",
            "Epoch 64/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.8394\n",
            "Epoch 65/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.8105\n",
            "Epoch 66/150\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.7374\n",
            "Epoch 67/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.7086\n",
            "Epoch 68/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.6852\n",
            "Epoch 69/150\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 1.6858\n",
            "Epoch 70/150\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.6598\n",
            "Epoch 71/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.6712\n",
            "Epoch 72/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.6484\n",
            "Epoch 73/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.6217\n",
            "Epoch 74/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.5859\n",
            "Epoch 75/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.5895\n",
            "Epoch 76/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 1.5897\n",
            "Epoch 77/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 1.5707\n",
            "Epoch 78/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.5525\n",
            "Epoch 79/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 1.5272\n",
            "Epoch 80/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 1.5152\n",
            "Epoch 81/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 1.4925\n",
            "Epoch 82/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.4843\n",
            "Epoch 83/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 1.4721\n",
            "Epoch 84/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.4694\n",
            "Epoch 85/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.4709\n",
            "Epoch 86/150\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.4675\n",
            "Epoch 87/150\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.4361\n",
            "Epoch 88/150\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.4476\n",
            "Epoch 89/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.4364\n",
            "Epoch 90/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.4209\n",
            "Epoch 91/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.3981\n",
            "Epoch 92/150\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.3917\n",
            "Epoch 93/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.3815\n",
            "Epoch 94/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.3729\n",
            "Epoch 95/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 1.3778\n",
            "Epoch 96/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 1.3794\n",
            "Epoch 97/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.3687\n",
            "Epoch 98/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 1.3608\n",
            "Epoch 99/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 1.3274\n",
            "Epoch 100/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 1.2983\n",
            "Epoch 101/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.2911\n",
            "Epoch 102/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.2838\n",
            "Epoch 103/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 1.2845\n",
            "Epoch 104/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 1.2636\n",
            "Epoch 105/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.2373\n",
            "Epoch 106/150\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.2166\n",
            "Epoch 107/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 1.1955\n",
            "Epoch 108/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 1.1950\n",
            "Epoch 109/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.2129\n",
            "Epoch 110/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.2359\n",
            "Epoch 111/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 1.2471\n",
            "Epoch 112/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.2135\n",
            "Epoch 113/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 1.1948\n",
            "Epoch 114/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 1.1710\n",
            "Epoch 115/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.1946\n",
            "Epoch 116/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 1.2050\n",
            "Epoch 117/150\n",
            "61/61 [==============================] - 14s 227ms/step - loss: 1.1994\n",
            "Epoch 118/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 1.2165\n",
            "Epoch 119/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 1.2194\n",
            "Epoch 120/150\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.2380\n",
            "Epoch 121/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.2237\n",
            "Epoch 122/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.2027\n",
            "Epoch 123/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.1662\n",
            "Epoch 124/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.1394\n",
            "Epoch 125/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.1029\n",
            "Epoch 126/150\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.0577\n",
            "Epoch 127/150\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.0348\n",
            "Epoch 128/150\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.0318\n",
            "Epoch 129/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.0179\n",
            "Epoch 130/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.0050\n",
            "Epoch 131/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 0.9918\n",
            "Epoch 132/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.0132\n",
            "Epoch 133/150\n",
            "61/61 [==============================] - 14s 221ms/step - loss: 1.0119\n",
            "Epoch 134/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 1.0199\n",
            "Epoch 135/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.0377\n",
            "Epoch 136/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 1.0764\n",
            "Epoch 137/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 1.0875\n",
            "Epoch 138/150\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 1.0785\n",
            "Epoch 139/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.1274\n",
            "Epoch 140/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.1519\n",
            "Epoch 141/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.1625\n",
            "Epoch 142/150\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 1.1857\n",
            "Epoch 143/150\n",
            "61/61 [==============================] - 14s 227ms/step - loss: 1.1646\n",
            "Epoch 144/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.1175\n",
            "Epoch 145/150\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.0737\n",
            "Epoch 146/150\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.0272\n",
            "Epoch 147/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 0.9951\n",
            "Epoch 148/150\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 0.9630\n",
            "Epoch 149/150\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 0.9247\n",
            "Epoch 150/150\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 0.9135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model \n",
        "import numpy as np \n",
        "import pickle \n",
        "\n",
        "\n",
        "# Load the model and tokenizer \n",
        "model = load_model('netword1.h5') \n",
        "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb')) \n",
        " \n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text): \n",
        "  \"\"\" In this function we are using the tokenizer and models trained\n",
        "      and we are creating the sequence of the text entered and then\n",
        "      using our model to predict and return the the predicted word.\"\"\" \n",
        "\n",
        "  for i in range(3):\n",
        "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "    sequence = np.array(sequence) \n",
        "    # preds = model.predict_classes(sequence) \n",
        "    pred=model.predict(sequence)\n",
        "    preds=np.argmax(pred,axis=1)\n",
        "    # print(preds) \n",
        "    predicted_word = \"\" \n",
        "    for key, value in tokenizer.word_index.items():\n",
        "      if value == preds: \n",
        "        predicted_word = key\n",
        "        break\n",
        "\n",
        "    print(predicted_word) \n",
        "    return predicted_word \n",
        "\n",
        " \n",
        "\n",
        "\"\"\" We are testing our model and we will run the model\n",
        "    until the user decides to stop the script. \n",
        "    While the script is running we try and check if\n",
        "    the prediction can be made on the text. If no\n",
        "    prediction can be made we just continue.\"\"\" \n",
        "\n",
        " \n",
        "\n",
        "# text1 = \"at the dull\"\n",
        "# text2 = \"collection of textile\"\n",
        "# text3 = \"what a strenuous\" \n",
        "# text4 = \"stop the script\" \n",
        "\n",
        "\n",
        "while(True):\n",
        "  text = input(\"Enter your line: \") \n",
        "\n",
        "  if text == \"stop the script\":\n",
        "    print(\"Ending The Program.....\") \n",
        "    break \n",
        "  \n",
        "  else:\n",
        "    try: \n",
        "      text = text.split(\" \") \n",
        "      text = text[-1] \n",
        "      text = ''.join(text) \n",
        "      Predict_Next_Words(model, tokenizer, text) \n",
        "\n",
        "    except: \n",
        "      continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MKcr3VT-tbu",
        "outputId": "35de91b7-2c25-4104-a5e3-a54f6b10762d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your line: at the dull\n",
            "weather\n",
            "Enter your line: collection of textile\n",
            "samples\n",
            "Enter your line: what a strenuous\n",
            "career\n",
            "Enter your line: stop the script\n",
            "Ending The Program.....\n"
          ]
        }
      ]
    }
  ]
}